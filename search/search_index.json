{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the EDANSA Bioacoustics Model Documentation","text":"<p>This documentation provides information on using the pre-trained bioacoustic models developed as part of the EDANSA project.</p>"},{"location":"#overview","title":"Overview","text":"<p>Passive Acoustic Monitoring (PAM) generates vast amounts of audio data, requiring efficient automated methods for analysis. The models presented here are designed to detect various bioacoustic events (like general bird song, insect sounds, or anthropophony) and specific bird species calls within audio recordings.</p> <p>We provide two primary pre-trained models ready for use:</p> <ol> <li>General Bioacoustics Model (<code>31m2plxv-V1</code>): Detects broader categories of sound events relevant to ecological monitoring.</li> <li>Bird Species Model (<code>ppq7zxqq</code>): Focused on identifying specific bird species commonly found in North American arctic environments.</li> </ol>"},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li>Installation: Begin by following the Installation Guide to set up the necessary environment and dependencies.</li> <li>Using a Model: Learn how to run inference with the pre-trained models:<ul> <li>Running Inference: Command-line arguments and examples.</li> <li>Providing Audio Data: How to format your input.</li> <li>Understanding Results: Explanation of the output CSV files.</li> </ul> </li> <li>Model Details: Find specific performance metrics and details for each model:<ul> <li>General Model Card</li> <li>Bird Species Model Card</li> </ul> </li> </ul>"},{"location":"#troubleshooting","title":"Troubleshooting","text":"<p>Encountering issues? Check the Troubleshooting Guide. </p>"},{"location":"installation/","title":"Installation Guide","text":"<p>This guide covers how to set up the necessary environment and install the EDANSA package to run the pre-trained models.</p>"},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Git: You need Git installed to clone the repository. You can download it from git-scm.com.</li> <li>(Recommended) Conda: We strongly recommend using Conda (or Miniconda/Mamba/Micromamba) for managing environments and dependencies. See the Conda documentation for installation details if needed.</li> <li>(Optional) Python: If not using Conda, you'll need Python installed (version &gt;= 3.8 and &lt; 3.12).</li> <li>(GPU Users) NVIDIA Drivers: If you intend to use a GPU, ensure you have appropriate NVIDIA drivers installed before creating the conda environment.</li> </ul>"},{"location":"installation/#step-1-clone-the-repository","title":"Step 1: Clone the Repository","text":"<p>First, clone this repository to your local machine:</p> <pre><code>git clone https://github.com/speechLabBcCuny/EDANSA.git\ncd EDANSA\n</code></pre>"},{"location":"installation/#step-2-install-dependencies","title":"Step 2: Install Dependencies","text":"<p>Choose one of the following methods (Conda is recommended).</p>"},{"location":"installation/#method-1-using-conda-recommended","title":"Method 1: Using Conda (Recommended)","text":"<p>This method uses the provided <code>environment.yml</code> file, which specifies dependencies primarily from the <code>conda-forge</code> channel following PyTorch's recommendations.</p> <ol> <li> <p>Install Conda: If you don't have Conda, install Miniconda or the full Anaconda.</p> </li> <li> <p>Create Environment: Create a new Conda environment using the <code>environment.yml</code> file. You must specify a name for your new environment using the <code>-n</code> flag. The file is configured to request the standard <code>pytorch</code> package from <code>conda-forge</code>. Conda's solver should automatically detect your system configuration and install the appropriate version (CPU or GPU-enabled with CUDA).</p> <pre><code># Replace &lt;your_env_name&gt; with your desired environment name (e.g., edansa)\nconda env create -f environment.yml -n &lt;your_env_name&gt;\n</code></pre> <p>Alternatively: Updating an Existing Environment If you already have a Conda environment activated that you wish to use, you can update it with the required packages. Ensure the target environment is activated before running the update command: <pre><code># Activate your existing environment first\n# conda activate your-env-name\n\n# Using conda\nconda env update --file environment.yml\n\n# Or using micromamba\nmicromamba env update -f environment.yml\n</code></pre></p> </li> <li> <p>Activate Environment: Activate the environment using the name you chose during creation:     <pre><code>conda activate &lt;your_env_name&gt; \n</code></pre>     You need to activate this environment every time you want to run the EDANSA scripts.</p> </li> </ol>"},{"location":"installation/#method-2-using-pip","title":"Method 2: Using Pip","text":"<p>If you prefer not to use Conda, you can use <code>pip</code> with a virtual environment. This method requires installing PyTorch separately according to your system's configuration (CPU or specific CUDA version) before installing the other requirements.</p> <ol> <li> <p>Create Virtual Environment (Recommended): <pre><code>python -m venv .venv \nsource .venv/bin/activate # On Windows use `.venv\\\\Scripts\\\\activate`\n</code></pre></p> </li> <li> <p>Install PyTorch: </p> <ul> <li>Visit the official PyTorch website installation guide.</li> <li>Select your operating system (Linux, Mac, Windows), package manager (<code>Pip</code>), compute platform (CPU or the specific CUDA version compatible with your drivers), and Python version.</li> <li>Copy the generated <code>pip install</code> command (it will likely include <code>--index-url</code> or <code>--extra-index-url</code>).</li> <li>Run that command in your activated virtual environment. For example:     <pre><code># Example for CPU-only on Linux/Mac:\npip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n\n# Example for CUDA 12.1 on Linux/Windows:\n# pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 \n</code></pre></li> <li>Important: You must ensure you have the correct NVIDIA drivers and CUDA toolkit installed manually if you choose a CUDA option. <code>pip</code> will only install the PyTorch library compiled for that CUDA version, not the toolkit itself.</li> </ul> </li> <li> <p>Install Remaining Dependencies: Once PyTorch is installed correctly for your system, install the other required packages using the <code>requirements.txt</code> file:     <pre><code>pip install -r requirements.txt\n</code></pre> <code>pip</code> should detect that PyTorch, torchvision, and torchaudio are already installed and handle the rest.</p> <p>Note: Managing PyTorch versions and CUDA compatibility can sometimes be complex with <code>pip</code>. Conda often handles this more automatically.</p> </li> </ol>"},{"location":"installation/#step-2b-audio-backend-for-torchaudio","title":"Step 2b: Audio Backend for Torchaudio","text":"<p><code>torchaudio</code>, the library used for loading audio files, requires an external backend library to perform the actual decoding. Supported backends include FFmpeg (recommended, cross-platform), SoX (Linux/macOS), and SoundFile. [torchaudio Backends Documentation]</p> <ul> <li>Conda Users: The provided <code>environment.yml</code> file includes <code>ffmpeg</code>. This backend should be installed automatically when you create the environment.</li> <li> <p>Pip Users: If you install dependencies using <code>pip</code> (Method 2), you might need to install <code>ffmpeg</code> or another backend (<code>libsndfile</code> for SoundFile, <code>sox</code>) separately using your system's package manager (e.g., <code>apt</code>, <code>brew</code>, <code>yum</code>) if it's not already present on your system.</p> </li> <li> <p>Checking Available Backends: Once you have installed dependencies and activated your environment (e.g., <code>&lt;your_env_name&gt;</code>), you can check which backends <code>torchaudio</code> can detect by running the following Python command:</p> <p><pre><code>python -c \"import torchaudio; print(torchaudio.list_audio_backends())\"\n</code></pre> Ensure that the output lists at least one available backend (e.g., <code>'ffmpeg'</code>, <code>'sox'</code>, <code>'soundfile'</code>).</p> </li> </ul>"},{"location":"installation/#step-3-install-the-edansa-package","title":"Step 3: Install the EDANSA Package","text":"<p>After installing the dependencies using either Conda or Pip, you need to install the <code>edansa</code> package itself. Make sure your environment (e.g., <code>&lt;your_env_name&gt;</code>) is activated, and then run the following command from the root directory of the cloned repository (<code>EDANSA/</code>):</p> <pre><code>pip install -e .\n</code></pre>"},{"location":"installation/#installation-complete","title":"Installation Complete","text":"<p>You should now have the necessary environment and the <code>edansa</code> package installed. You can proceed to run inference using the pre-trained models as described in the Running Inference guide.</p>"},{"location":"common/troubleshooting/","title":"Troubleshooting Guide","text":"<p>This page lists common issues encountered when installing or running the EDANSA inference scripts and suggests potential solutions.</p>"},{"location":"common/troubleshooting/#installation-issues","title":"Installation Issues","text":""},{"location":"common/troubleshooting/#conda-environment-creation-fails","title":"Conda Environment Creation Fails","text":"<ul> <li>Symptom: <code>conda env create -f environment.yml -n &lt;name&gt;</code> fails, often with messages about unsatisfiable dependencies or package conflicts.</li> <li>Possible Causes &amp; Solutions:<ul> <li>Outdated Conda/Mamba: Ensure your Conda or Mamba installation is up-to-date: <code>conda update conda</code> or <code>mamba update mamba</code>.</li> <li>Channel Conflicts: Sometimes channel priorities can cause issues. Try creating the environment strictly from <code>conda-forge</code> (though this might miss specific builds):     <pre><code>conda create -n &lt;your_env_name&gt; --channel conda-forge --file environment.yml \n</code></pre></li> <li>Network Issues: Temporary network problems can prevent downloading packages. Try again later.</li> <li>Incompatible System Dependencies (GPU): If creating the GPU environment (<code>environment.gpu.yml</code>, if using the older two-file approach) fails, ensure your NVIDIA drivers are compatible with the requested CUDA toolkit version (e.g., 12.1). Update drivers if necessary.</li> </ul> </li> </ul>"},{"location":"common/troubleshooting/#pip-installation-fails-pytorch","title":"Pip Installation Fails (PyTorch)","text":"<ul> <li>Symptom: Running the specific <code>pip install torch ...</code> command from the PyTorch website fails.</li> <li>Possible Causes &amp; Solutions:<ul> <li>Incorrect Command: Double-check that you copied the exact command matching your OS, package manager (Pip), and compute platform (CPU/CUDA version) from pytorch.org.</li> <li>Unsupported CUDA Version: Ensure the CUDA version selected on the PyTorch website matches the CUDA toolkit version compatible with your NVIDIA drivers.</li> <li>Network Issues: Try again later.</li> <li>Permissions: Ensure you have permissions to install packages in the target environment.</li> </ul> </li> </ul>"},{"location":"common/troubleshooting/#pip-installation-fails-pip-install-r-requirementstxt","title":"Pip Installation Fails (<code>pip install -r requirements.txt</code>)","text":"<ul> <li>Symptom: Fails after successfully installing PyTorch.</li> <li>Possible Causes &amp; Solutions:<ul> <li>Missing PyTorch: Ensure you successfully completed the PyTorch installation step before running <code>pip install -r requirements.txt</code>.</li> <li>Network Issues: Try again later.</li> </ul> </li> </ul>"},{"location":"common/troubleshooting/#importerror-edansa-or-module-not-found","title":"<code>ImportError: edansa</code> or Module Not Found","text":"<ul> <li>Symptom: Running the inference script fails with an <code>ImportError</code> for <code>edansa</code> or one of its submodules.</li> <li>Possible Causes &amp; Solutions:<ul> <li>Environment Not Activated: You forgot to activate the correct Conda or Pip virtual environment before running the script. Activate it (e.g., <code>conda activate &lt;your_env_name&gt;</code> or <code>source .venv/bin/activate</code>).</li> <li>Package Not Installed: You missed Step 3 of the installation: <code>pip install -e .</code>. Run this command from the root directory of the repository (<code>EDANSA/</code>) while your environment is activated.</li> </ul> </li> </ul>"},{"location":"common/troubleshooting/#torchaudio-backend-error-cannot-load-audio","title":"<code>torchaudio</code> Backend Error / Cannot Load Audio","text":"<ul> <li>Symptom: Errors related to loading audio files, potentially mentioning missing backends like FFmpeg or SoX.</li> <li>Possible Causes &amp; Solutions:<ul> <li>Missing Backend: <code>torchaudio</code> needs a backend library like FFmpeg to load audio. <ul> <li>If using Conda with <code>environment.yml</code>, <code>ffmpeg</code> should have been installed. Verify using <code>python -c \"import torchaudio; print(torchaudio.list_audio_backends())\"</code>. If <code>ffmpeg</code> isn't listed, try reinstalling the environment or explicitly installing ffmpeg: <code>conda install ffmpeg -c conda-forge</code>.</li> <li>If using Pip, you likely need to install <code>ffmpeg</code> (or <code>sox</code>, <code>libsndfile</code>) using your system's package manager (e.g., <code>sudo apt update &amp;&amp; sudo apt install ffmpeg</code>, <code>brew install ffmpeg</code>).</li> </ul> </li> <li>Corrupt Audio File: The specific audio file might be corrupted or incomplete.</li> <li>Unsupported Format (for Pip): If using Pip, ensure the audio format is supported by the backend you have installed.</li> </ul> </li> </ul>"},{"location":"common/troubleshooting/#runtime-errors","title":"Runtime Errors","text":""},{"location":"common/troubleshooting/#cuda-errors-gpu-usage","title":"CUDA Errors (GPU Usage)","text":"<ul> <li>Symptom: Errors mentioning CUDA, <code>RuntimeError: CUDA error: out of memory</code>, <code>AssertionError: Torch not compiled with CUDA enabled</code>, or devices not found.</li> <li>Possible Causes &amp; Solutions:<ul> <li>Incorrect Environment: You might have installed the CPU-only version of PyTorch but are trying to run on a GPU (or specified <code>--device cuda</code>). Ensure you installed using the GPU instructions (e.g., via <code>environment.yml</code> on a GPU machine or the correct <code>pip</code> command).</li> <li>GPU Memory: The model or batch size requires more GPU memory than available. Try reducing batch size (if applicable to the script/config) or running on a GPU with more memory. This model may require ~5GB+ VRAM.</li> <li>Incorrect Device Specified: Check the <code>--device</code> argument (<code>cuda</code>, <code>cuda:0</code>, <code>cpu</code>). Ensure it matches your available hardware.</li> <li>Driver/CUDA Toolkit Mismatch: The installed NVIDIA drivers might be incompatible with the CUDA toolkit version PyTorch was compiled against. Update your drivers.</li> <li>Environment Not Activated: Ensure the correct conda/pip environment with GPU PyTorch is activated.</li> </ul> </li> </ul>"},{"location":"common/troubleshooting/#file-not-found-errors","title":"File Not Found Errors","text":"<ul> <li>Symptom: Errors indicating <code>--model_path</code>, <code>--config_file</code>, <code>--input_folder</code>, <code>--input_files_list</code>, or <code>--output_folder</code> cannot be found.</li> <li>Possible Causes &amp; Solutions:<ul> <li>Typo: Double-check the spelling and case sensitivity of the path.</li> <li>Incorrect Path: Ensure the path is correct relative to your current working directory, or use an absolute path.</li> <li>File/Folder Missing: Verify that the specified file or folder actually exists at that location.</li> <li>Permissions: Ensure you have read permissions for input files/folders and write permissions for the output folder.</li> </ul> </li> </ul>"},{"location":"common/troubleshooting/#errors-during-inference-_process_single_audio_file","title":"Errors During Inference (<code>_process_single_audio_file</code>)","text":"<ul> <li>Symptom: The script processes some files but fails on others, often logging errors to <code>failed_files.csv</code> in the output folder.</li> <li>Possible Causes &amp; Solutions:<ul> <li>Corrupt/Invalid Audio File: The audio file might be zero-length, corrupted, or in an unexpected format not handled correctly by the backend.</li> <li>Memory Issues (RAM/CPU): Very long audio files could potentially consume large amounts of RAM during loading or processing.</li> <li>Channel Selection Error: An invalid <code>--channel_selection_method</code> was provided, or the chosen channel index doesn't exist.</li> <li>Unexpected Audio Properties: Files with unusual sample rates or channel counts might cause issues if not handled correctly by resampling or channel selection.</li> <li>Check <code>failed_files.csv</code>: This file in your output directory contains the specific error message for each failed file, which is crucial for diagnosis.</li> </ul> </li> </ul>"},{"location":"common/troubleshooting/#unexpected-output","title":"Unexpected Output","text":""},{"location":"common/troubleshooting/#timestamps-are-relative-seconds-instead-of-absolute","title":"Timestamps are Relative (Seconds) Instead of Absolute","text":"<ul> <li>Symptom: The <code>timestamp</code> column in the output CSV shows <code>0.0</code>, <code>10.0</code>, <code>20.0</code>, etc., instead of dates and times.</li> <li>Cause: The input audio filenames did not match the expected <code>recorderid_YYYYMMDD_HHMMSS</code> or <code>YYYYMMDD_HHMMSS</code> format.</li> <li>Solution: This is expected behavior if filenames aren't parsable. If you need absolute timestamps, rename your input files according to the convention described in the Providing Audio Data section before running inference.</li> </ul>"},{"location":"common/troubleshooting/#low-confidence-scores-poor-performance","title":"Low Confidence Scores / Poor Performance","text":"<ul> <li>Symptom: The model produces very low confidence scores or seems to perform poorly on your data.</li> <li>Possible Causes &amp; Solutions:<ul> <li>Domain Mismatch: The acoustic characteristics of your data (environment, noise, species vocalizations) might be significantly different from the model's training data.</li> <li>Incorrect Model: Ensure you are using the correct model (<code>general</code> or <code>bird_species</code>) for your target sounds.</li> <li>High Noise / Clipping: Check the <code>clipping</code> column in the output. High noise or clipping can degrade performance.</li> <li>Thresholding: Default thresholds might not be optimal for your specific data or goals. Refer to the Model Card for recommended starting thresholds and consider adjusting them based on your own evaluation.</li> <li>Audio Quality: Poor quality recordings (low sample rate, high compression artifacts if using lossy formats) can affect results.</li> </ul> </li> </ul>"},{"location":"using_pretrained_model/","title":"Running Inference with a Pretrained Model","text":"<p>This document explains how to use the inference script (<code>runs/augment/inference.py</code>) to generate predictions from audio files using a pretrained EDANSA model.</p>"},{"location":"using_pretrained_model/#basic-usage","title":"Basic Usage","text":"<p>The core command structure involves specifying the model, its configuration, the input audio source, and the desired output location.</p> <pre><code>python runs/augment/inference.py \\\n    --model_path &lt;path_to_model.pt&gt; \\\n    --config_file &lt;path_to_config.json&gt; \\\n    --input_folder &lt;path_to_your_audio_folder&gt; \\\n    --output_folder &lt;path_to_save_results&gt;\n</code></pre>"},{"location":"using_pretrained_model/#example-running-the-main-edansa-model-id-31m2plxv-v1","title":"Example: Running the Main EDANSA Model (ID: 31m2plxv-V1)","text":"<p>Here is a concrete example using the primary pre-trained EDANSA model included in the <code>assets</code> directory. This command assumes:</p> <ul> <li>You are running the command from the root directory of the <code>EDANSA-2019</code> repository.</li> <li>Your audio files are located in a directory named <code>my_audio_files</code>.</li> <li>You want to save the results to a directory named <code>inference_results</code>.</li> </ul> <pre><code>python runs/augment/inference.py \\\n    --model_path assets/31m2plxv-V1/model_info/best_model_370_val_f1_min=0.8028.pt \\\n    --config_file assets/31m2plxv-V1/model_info/model_config.json \\\n    --input_folder my_audio_files/ \\\n    --output_folder inference_results/\n</code></pre>"},{"location":"using_pretrained_model/#command-line-arguments","title":"Command-Line Arguments","text":"<p>The inference script accepts several arguments to control its behavior:</p>"},{"location":"using_pretrained_model/#required-arguments","title":"Required Arguments","text":"<ul> <li><code>--model_path &lt;path&gt;</code>: Required. Path to the trained model checkpoint file (e.g., <code>.pth</code>).</li> <li><code>--config_file &lt;path&gt;</code> / <code>-c &lt;path&gt;</code>: Required. Path to the JSON configuration file associated with the model. This file contains essential parameters like sampling rate, class labels, and excerpt length used during training.</li> </ul>"},{"location":"using_pretrained_model/#input-source-choose-one","title":"Input Source (Choose ONE)","text":"<p>You must specify one of the following options to provide the audio files for inference:</p> <ul> <li><code>--input_files_list &lt;path&gt;</code>: Path to a text file where each line contains the full path to an audio file to be processed.</li> <li><code>--input_folder &lt;path&gt;</code>: Path to a folder containing audio files. The script will recursively search this folder for audio files. Officially supported and tested formats are WAV (<code>.wav</code>) and FLAC (<code>.flac</code>). While it may attempt to load other formats like MP3, OGG, or AIFF, these are not guaranteed to work correctly.</li> </ul>"},{"location":"using_pretrained_model/#output-control","title":"Output Control","text":"<ul> <li><code>--output_folder &lt;path&gt;</code> / <code>-O &lt;path&gt;</code>: Directory where the prediction files will be saved.<ul> <li>If not specified, defaults to a folder named <code>outputs</code> in the current working directory.</li> <li>The script will create one CSV file per input audio file. It creates subdirectories within the output folder that mirror the structure of the input folder or the paths provided in the input list.</li> </ul> </li> </ul>"},{"location":"using_pretrained_model/#execution-environment","title":"Execution Environment","text":"<ul> <li><code>--device &lt;device_name&gt;</code>: Specify the computational device. Examples: <code>'cpu'</code>, <code>'cuda'</code>, <code>'cuda:0'</code>.<ul> <li>If not specified, the script defaults to <code>'cuda'</code> if a CUDA-compatible GPU is detected, otherwise it uses <code>'cpu'</code>.</li> </ul> </li> </ul>"},{"location":"using_pretrained_model/#output-format","title":"Output Format","text":"<ul> <li>Predictions: Results are saved as CSV files, with one file generated for each input audio file.<ul> <li>The directory structure within the <code>--output_folder</code> will mirror the structure of the input source (either the <code>--input_folder</code> or the paths from <code>--input_files_list</code>).</li> <li>Each CSV file contains:<ul> <li>Timestamps for each prediction segment (either absolute datetime if parsed from filename, or relative seconds).</li> <li>Confidence scores per target class defined in the config file.</li> <li>Clipping percentage per segment (if not skipped via <code>--skip_clipping_info</code>).</li> </ul> </li> </ul> </li> </ul>"},{"location":"using_pretrained_model/#error-handling","title":"Error Handling","text":"<p>If the script encounters an error while processing a specific audio file (e.g., loading error, processing error, save error), it will:</p> <ol> <li>Log the error message to the console/log output.</li> <li>Record the failed file path and the error message in a CSV file named <code>failed_files.csv</code> located within the specified <code>--output_folder</code>. This allows you to easily identify and investigate problematic files after a large batch run.</li> </ol>"},{"location":"using_pretrained_model/#advanced-settings","title":"Advanced Settings","text":""},{"location":"using_pretrained_model/#audio-processing-options","title":"Audio Processing Options","text":"<ul> <li><code>--channel_selection_method &lt;method&gt;</code>: Specifies how to handle multi-channel (e.g., stereo) audio files. Options are:<ul> <li><code>'average'</code> (Default): Averages the channels to create a mono signal.</li> <li><code>'clipping'</code>: Selects the channel with the least clipping per segment (defined by <code>excerpt_length</code> in the model config). Requires valid clipping data calculation. Falls back to <code>'average'</code> if clipping calculation fails or data is invalid.</li> <li><code>'channel_N'</code>: Selects a specific channel by its index (e.g., <code>'channel_0'</code>, <code>'channel_1'</code>).</li> </ul> </li> <li><code>--skip_clipping_info</code>: If this flag is present, the script will not calculate or include the percentage of clipped samples in the output results for prediction files. By default (flag absent), clipping information is calculated and included if possible.</li> </ul>"},{"location":"using_pretrained_model/#optional-processing-behavior","title":"Optional Processing &amp; Behavior","text":"<ul> <li><code>--force_overwrite</code>: If this flag is present, the script will process all input files, even if a corresponding output file already exists in the output folder.<ul> <li>By default (flag absent), the script checks for existing output files and skips processing for files where the output already exists.</li> </ul> </li> </ul>"},{"location":"using_pretrained_model/model_card_bird_species_ppq7zxqq/","title":"Model Card: Bird Species Model (ppq7zxqq)","text":"<p>This document provides details about the pre-trained EDANSA model specifically tuned for North American arctic bird species.</p>"},{"location":"using_pretrained_model/model_card_bird_species_ppq7zxqq/#model-details","title":"Model Details","text":"<ul> <li>Model ID / Run Name: <code>ppq7zxqq</code></li> <li>Description: A bioacoustics model trained to detect specific bird species commonly found in North American arctic environments.</li> </ul>"},{"location":"using_pretrained_model/model_card_bird_species_ppq7zxqq/#assets-location","title":"Assets Location","text":"<p>The model weights, configuration file, recommended thresholds, and raw predictions for threshold tuning are located within this repository:</p> <ul> <li>Model Weights: <code>assets/ppq7zxqq/best_model_200_val_f1_min=0.7857_ppq7zxqq.pt</code></li> <li>Configuration File: <code>assets/ppq7zxqq/model_config_ppq7zxqq.json</code></li> <li>Recommended Thresholds (Test Set): <code>assets/ppq7zxqq/ppq7zxqq_thresholds.csv</code> (Contains optimal F1 thresholds derived from the test set - recommended starting points)</li> <li>Raw Predictions (for custom thresholding): <code>assets/ppq7zxqq/all_predictions_epoch_200_ppq7zxqq.csv</code> (Contains raw model outputs/logits. These need transformation, e.g., via a sigmoid function, to be interpreted as probabilities between 0 and 1. Useful for advanced users wanting to determine custom thresholds).</li> </ul>"},{"location":"using_pretrained_model/model_card_bird_species_ppq7zxqq/#performance-metrics-test-set","title":"Performance Metrics (Test Set)","text":"<p>The <code>Short Label</code> corresponds to the prefix used in output CSV column headers (e.g., <code>pred_LALO</code>).</p> Label (Species) Short Label Test AUC Test F1 Score F1 Threshold Lapland Longspur LALO 0.974 0.909 0.433 White-crowned Sparrow WCSP 0.981 0.864 0.635 American Tree Sparrow ATSP 0.991 0.835 0.222 Willow Ptarmigan WIPT 0.942 0.802 0.701 Savannah Sparrow SAVS 0.938 0.797 0.578 Common Redpoll CORE 0.989 0.786 0.197 <p>Metric Definitions:</p> <ul> <li>AUC (Area Under the ROC Curve): Measures the model's ability to distinguish between classes across all possible thresholds. A value closer to 1.0 indicates better separability.</li> <li>F1 Score: The harmonic mean of precision and recall at the specified F1 Threshold. It provides a balance between finding relevant sounds (recall) and ensuring the found sounds are indeed relevant (precision). A value closer to 1.0 is better.</li> <li>F1 Threshold: The confidence score threshold optimized on the test set to achieve the reported F1 Score. Use this as a starting point for thresholding predictions.</li> </ul>"},{"location":"using_pretrained_model/model_card_bird_species_ppq7zxqq/#intended-use","title":"Intended Use","text":"<p>This model is intended for detecting specific bird species in passive acoustic monitoring data from relevant geographic regions (e.g., North American arctic/sub-arctic). It can be used for species occurrence studies or as input for further analysis.</p>"},{"location":"using_pretrained_model/model_card_bird_species_ppq7zxqq/#limitations-thresholding","title":"Limitations &amp; Thresholding","text":"<ul> <li>Performance may vary depending on the specific acoustic environment, recorder type, noise levels, distance to the sound source, and regional variations in vocalizations.</li> <li>The model's performance on species not included in the training data is unknown.</li> <li>Threshold Adjustment: The optimal confidence score thresholds provided in the <code>ppq7zxqq_thresholds.csv</code> file are recommended starting points. Users should evaluate and potentially adjust these thresholds based on their specific project goals (e.g., prioritizing recall over precision) and test data.</li> <li>Advanced Thresholding: For users wanting to perform detailed threshold analysis or optimize using different metrics, the raw model predictions (logits, which need transformation like sigmoid to become 0-1 probabilities) are available in <code>all_predictions_epoch_200_ppq7zxqq.csv</code>. </li> </ul>"},{"location":"using_pretrained_model/model_card_general_31m2plxv-V1/","title":"Model Card: EDANSA Bioacoustics Model (31m2plxv-V1)","text":"<p>This document provides details about the primary pre-trained EDANSA model.</p>"},{"location":"using_pretrained_model/model_card_general_31m2plxv-V1/#model-details","title":"Model Details","text":"<ul> <li>Model ID: <code>31m2plxv-V1</code></li> <li>Description: A general-purpose bioacoustics model trained to detect various sound event categories relevant to ecological monitoring.</li> </ul>"},{"location":"using_pretrained_model/model_card_general_31m2plxv-V1/#assets-location","title":"Assets Location","text":"<p>The model weights, configuration file, recommended thresholds, and raw predictions for threshold tuning are located within this repository:</p> <ul> <li>Model Weights: <code>assets/31m2plxv-V1/model_info/best_model_370_val_f1_min=0.8028.pt</code></li> <li>Configuration File: <code>assets/31m2plxv-V1/model_info/model_config.json</code></li> <li>Recommended Thresholds (Validation Set): <code>assets/31m2plxv-V1/model_info/thresholds_31m2plxv_epoch370.csv</code> (Contains optimal thresholds derived from the validation set - recommended starting points for users)</li> <li>Raw Predictions (for custom thresholding): <code>assets/31m2plxv-V1/model_info/datasetV5.4.10_modelcard.csv</code> (Contains raw model outputs, often called logits. These need transformation, e.g., via a sigmoid function, to be interpreted as probabilities between 0 and 1. Useful for advanced users wanting to determine custom thresholds).</li> </ul>"},{"location":"using_pretrained_model/model_card_general_31m2plxv-V1/#performance-metrics-test-set","title":"Performance Metrics (Test Set)","text":"<p>The following metrics were evaluated on an independent test set, providing an estimate of the model's performance on unseen data. The F1 scores reported here were calculated by applying the optimal thresholds. The <code>Short Label</code> corresponds to the prefix used in output CSV column headers (e.g., <code>pred_Bio</code>).</p> Label (Class) Short Label Test AUC Test F1 Score Biophony Bio 0.973 0.935 Bird (Generic) Bird 0.973 0.920 Songbird SongB 0.957 0.829 Duck/Goose/Swan DGS 0.925 0.591 Grouse Grous 0.948 0.763 Insect Bug 0.969 0.902 Anthropophony Anth 0.995 0.976 Aircraft Airc 0.967 0.850 Silence Sil 0.957 0.789 <p>Metric Definitions:</p> <ul> <li>AUC (Area Under the ROC Curve): Measures the model's ability to distinguish between classes across all possible thresholds. A value closer to 1.0 indicates better separability.</li> <li>F1 Score: The harmonic mean of precision and recall, calculated using the thresholds from the validation set CSV. It provides a balance between finding relevant sounds (recall) and ensuring the found sounds are indeed relevant (precision). A value closer to 1.0 is better.</li> </ul>"},{"location":"using_pretrained_model/model_card_general_31m2plxv-V1/#intended-use","title":"Intended Use","text":"<p>This model is intended for detecting bioacoustic events in passive acoustic monitoring data. It can serve as a baseline classifier or feature extractor for ecological studies.</p>"},{"location":"using_pretrained_model/model_card_general_31m2plxv-V1/#limitations-thresholding","title":"Limitations &amp; Thresholding","text":"<ul> <li>Performance may vary depending on the specific acoustic environment, recorder type, noise levels, and distance to the sound source.</li> <li>The model's performance on classes not included in the training data is unknown.</li> <li>Threshold Adjustment: The optimal confidence score thresholds provided in the <code>thresholds_31m2plxv_epoch370.csv</code> file (derived from the validation set) are recommended starting points. Users should evaluate and potentially adjust these thresholds based on their specific project goals (e.g., prioritizing recall over precision) and validation data.</li> <li>Advanced Thresholding: For users wanting to perform detailed threshold analysis or optimize using different metrics, the raw model predictions (logits, which need transformation like sigmoid to become 0-1 probabilities) are available in <code>datasetV5.4.10_modelcard.csv</code>. </li> </ul>"},{"location":"using_pretrained_model/output_explanation/","title":"Understanding the Inference Results","text":"<p>After running the inference script (<code>runs/augment/inference.py</code>), you will find CSV files in your specified output directory. Each CSV file corresponds to one input audio file and contains the model's predictions segment by segment.</p> <p>Here's a breakdown of the columns you'll typically find in the output CSV:</p>"},{"location":"using_pretrained_model/output_explanation/#columns-explained","title":"Columns Explained","text":"<ul> <li> <p><code>timestamp</code>: </p> <ul> <li>Indicates the start time of the prediction segment.</li> <li>If you followed the recommended filename convention for your input audio files, this column will contain absolute timestamps (e.g., <code>2023-10-27 14:30:00</code>).</li> <li>If the filename could not be parsed for a date and time, this column will contain relative timestamps in seconds from the beginning of the audio file (e.g., <code>0.0</code>, <code>10.0</code>, <code>20.0</code>).</li> <li>The duration of each segment is determined by the <code>excerpt_length</code> parameter in the model's configuration file (<code>--config_file</code>).</li> </ul> </li> <li> <p>Confidence Score Columns (e.g., <code>pred_Aircraft</code>, <code>pred_Songbird</code>, <code>pred_Bird</code>, ...):</p> <ul> <li>There will be one column for each target class the model was trained to detect. The class names are defined in the model's configuration file.</li> <li>The value in each column represents the model's confidence score (typically ranging from 0.0 to 1.0) that the corresponding class is present within that specific time segment.</li> <li>A score closer to 1.0 indicates higher confidence, while a score closer to 0.0 indicates lower confidence.</li> <li>You can use these scores to set a threshold for deciding whether a class is considered present (e.g., consider a class present if its score is &gt; 0.7).</li> </ul> </li> <li> <p><code>clipping</code>:</p> <ul> <li>This column shows the proportion (0.0 to 1.0) of audio samples that were clipped within that specific time segment.</li> <li>Audio clipping occurs when the amplitude (loudness) of the sound exceeds the maximum level that can be recorded or represented digitally. This results in distortion of the waveform, essentially chopping off the peaks.</li> <li>A high clipping proportion (e.g., &gt; 0.01-0.05, corresponding to 1-5%) might indicate:<ul> <li>The recording gain was set too high.</li> <li>There were very loud sounds (potentially non-target noise like wind, rain, or handling noise) close to the microphone.</li> <li>Potential data quality issues for that segment.</li> </ul> </li> <li>While the model processes these segments, high clipping can sometimes affect prediction accuracy. This column provides context for interpreting the results.</li> <li>This column will contain <code>NaN</code> (Not a Number) or be absent if clipping calculation was skipped using the <code>--skip_clipping_info</code> flag during inference.</li> </ul> </li> </ul>"},{"location":"using_pretrained_model/output_explanation/#example-csv-snippet-with-absolute-timestamps","title":"Example CSV Snippet (with Absolute Timestamps)","text":"<pre><code>timestamp,pred_Aircraft,pred_Songbird,pred_Bird,clipping\n2023-07-15 04:00:00,0.85,0.05,0.10,0.002\n2023-07-15 04:00:10,0.88,0.03,0.12,0.001\n2023-07-15 04:00:20,0.15,0.75,0.05,0.015\n</code></pre> <p>In this example, the model is highly confident about Aircraft presence in the first two 10-second segments but detects Songbird with higher confidence in the third segment, which also shows a slightly higher clipping proportion (0.015 or 1.5%).</p>"},{"location":"using_pretrained_model/providing_audio_data/","title":"Providing Audio Data for Inference","text":"<p>This page describes how to provide your audio recordings to the inference script and explains how filenames and file formats can influence the process and results.</p>"},{"location":"using_pretrained_model/providing_audio_data/#input-methods-file-list-or-folder-scan","title":"Input Methods: File List or Folder Scan","text":"<p>You can provide audio files to the inference script (<code>runs/augment/inference.py</code>) in two flexible ways:</p> <ol> <li><code>--input_files_list &lt;path/to/list.txt&gt;</code>: Provide a plain text file where each line contains the absolute path to an audio file you want to process.<ul> <li>The files listed can be located anywhere on your system; they don't need to share a common directory structure.</li> </ul> </li> </ol> <pre><code>/path/to/my/recordings/region_A/site_01/2022/REC001_20220701_100000.flac\n/path/to/other/recordings/location_X/2023/AUDIOMOTH_20230515_083000.wav\n/another/drive/misc_audio/test_clip.flac\n...\n</code></pre> <ol> <li><code>--input_folder &lt;path/to/folder&gt;</code>: Provide the path to a directory. The script will recursively search this directory and all its subdirectories for audio files to process.<ul> <li>Officially supported and tested formats are WAV (<code>.wav</code>) and FLAC (<code>.flac</code>).</li> <li>While the script might attempt to load other formats (like MP3, OGG, AIFF) due to underlying libraries, using WAV or FLAC is strongly recommended for reliable results.</li> </ul> </li> </ol>"},{"location":"using_pretrained_model/providing_audio_data/#how-output-structure-mirrors-input","title":"How Output Structure Mirrors Input","text":"<p>Although the script doesn't require a specific input folder structure, it intelligently organizes the output files to correspond to your input organization.</p> <ol> <li>Determine Input Root: The script identifies a common base directory (the \"root\") from all the processed input file paths.</li> <li>Calculate Relative Path: For each input file, it determines its path relative to that common root.</li> <li>Create Mirrored Output: It saves the resulting prediction CSV under the directory specified by <code>--output_folder</code>, recreating the relative path structure from the input root.</li> </ol> <p>Example:</p> <p>If <code>--output_folder</code> is <code>/data/edansa_outputs</code>:</p> <ul> <li> <p>Using <code>--input_files_list</code> with:</p> <ul> <li><code>/mnt/recordings/anwr/site_A/2022/REC01_20220601_000000.flac</code></li> <li><code>/mnt/recordings/dalton/site_X/2023/AM05_20230810_060000.flac</code></li> </ul> <p>The common root is likely <code>/mnt/recordings/</code>. Outputs appear as:</p> <ul> <li><code>/data/edansa_outputs/anwr/site_A/2022/REC01_20220601_000000.csv</code></li> <li><code>/data/edansa_outputs/dalton/site_X/2023/AM05_20230810_060000.csv</code></li> </ul> </li> <li> <p>Using <code>--input_folder /mnt/recordings</code> (where the above files reside in subdirs) yields the same output structure.</p> </li> </ul> <p>This mirroring ensures that your output results remain organized consistently with your input data, regardless of how you structure the input.</p>"},{"location":"using_pretrained_model/providing_audio_data/#generating-timestamps-in-output-optional-filename-convention","title":"Generating Timestamps in Output (Optional Filename Convention)","text":"<p>The inference script can process audio files with any filename. However, if you want the output CSV files to contain absolute timestamps (e.g., <code>2023-10-27 14:30:00</code>), you need to follow a specific filename convention.</p> <ul> <li> <p>If Filename Contains Recognizable Timestamp: The script attempts to parse the date and time from the filename stem (the part before the extension like <code>.wav</code> or <code>.flac</code>). If successful, the <code>timestamp</code> column in the output CSV will contain absolute timestamps for the start of each prediction segment.</p> <p>Example Output (Timestamp Parsed): <pre><code>timestamp,ClassA,ClassB,clipping\n2022-08-02_22-59-08,0.1213,0.0324,0.122\n2022-08-02_22-59-18,0.2013,0.0355,0.164\n...\n</code></pre></p> </li> <li> <p>If Filename Does Not Contain Recognizable Timestamp: If the script cannot parse a date and time from the filename, the <code>timestamp</code> column will contain relative time indices in seconds, starting from 0.0 for the beginning of the file.</p> <p>Example Output (Timestamp Not Parsed): <pre><code>timestamp,ClassA,ClassB,clipping\n0.0,0.1213,0.0324,0.122\n10.0,0.2013,0.0355,0.164\n...\n</code></pre></p> </li> </ul>"},{"location":"using_pretrained_model/providing_audio_data/#recommended-timestamp-formats-in-filename","title":"Recommended Timestamp Formats in Filename","text":"<p>For the script to recognize and parse absolute timestamps, the filename stem should follow one of these formats (using underscores <code>_</code> as separators):</p> <ul> <li>Format 1: <code>recorderid_YYYYMMDD_HHMMSS</code> Example: <code>S4A10297_20190504_120000.flac</code></li> <li>Format 2: <code>YYYYMMDD_HHMMSS</code> Example: <code>20190504_120000.wav</code></li> </ul> <p>Details:</p> <ul> <li><code>recorderid</code>: (Optional) Identifier for the device. Avoid underscores within the ID.</li> <li><code>YYYYMMDD</code>: Required 8 digits for a valid date.</li> <li><code>HHMMSS</code>: Required 6 digits for a valid 24-hour time.</li> </ul> <p>Filename Impact on Timestamps</p> <p>Using one of the recommended filename formats enables automatic absolute timestamps in the output. Any other filename format will result in relative timestamps (seconds from start).</p>"},{"location":"using_pretrained_model/providing_audio_data/#recommended-audio-file-formats","title":"Recommended Audio File Formats","text":"<ul> <li>Supported &amp; Recommended: WAV (<code>.wav</code>) and FLAC (<code>.flac</code>) are the officially supported and tested formats. We strongly recommend using one of these for reliable results.</li> <li>FLAC Advantage: FLAC offers lossless compression, reducing file size compared to WAV without quality loss, which is ideal for storage and transfer.</li> <li>Conversion: If your files are in other formats (e.g., MP3), consider converting them to WAV or FLAC using tools like <code>ffmpeg</code> or Audacity.</li> </ul> <p>Metadata During Conversion</p> <p>Standard conversion methods might not preserve all embedded metadata (e.g., GPS tags) from original files. This model does not use such metadata, but if you need it for other purposes, use conversion tools carefully.</p> <p>Basic Conversion with ffmpeg</p> <pre><code># Convert WAV to FLAC\nffmpeg -i input_audio.wav output_audio.flac\n\n# Convert MP3 to FLAC\nffmpeg -i input_audio.mp3 output_audio.flac \n</code></pre>"}]}